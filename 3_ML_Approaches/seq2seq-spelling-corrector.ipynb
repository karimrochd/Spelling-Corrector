{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10749064,"sourceType":"datasetVersion","datasetId":6666495}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport time\nimport random\n\nimport numpy as np\nimport pandas as pd\nfrom collections import namedtuple\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, random_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:04.511500Z","iopub.execute_input":"2025-02-18T22:35:04.511934Z","iopub.status.idle":"2025-02-18T22:35:04.517639Z","shell.execute_reply.started":"2025-02-18T22:35:04.511896Z","shell.execute_reply":"2025-02-18T22:35:04.516840Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def load_book(path):\n    \"\"\"Load a book from its file\"\"\"\n    input_file = os.path.join(path)\n    with open(input_file, encoding='utf-8', errors='replace') as f:\n        book = f.read()\n    return book","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:04.676086Z","iopub.execute_input":"2025-02-18T22:35:04.676358Z","iopub.status.idle":"2025-02-18T22:35:04.680254Z","shell.execute_reply.started":"2025-02-18T22:35:04.676337Z","shell.execute_reply":"2025-02-18T22:35:04.679511Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Collect all of the book file names\npath = '/kaggle/input/books-dataset/books/'\nbook_files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\nbook_files = book_files[1:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:04.838065Z","iopub.execute_input":"2025-02-18T22:35:04.838322Z","iopub.status.idle":"2025-02-18T22:35:04.869059Z","shell.execute_reply.started":"2025-02-18T22:35:04.838302Z","shell.execute_reply":"2025-02-18T22:35:04.868415Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Load the books using the file names\nbooks = []\nfor book in book_files:\n    books.append(load_book(path+book))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:04.991739Z","iopub.execute_input":"2025-02-18T22:35:04.992026Z","iopub.status.idle":"2025-02-18T22:35:05.019416Z","shell.execute_reply.started":"2025-02-18T22:35:04.992004Z","shell.execute_reply":"2025-02-18T22:35:05.018506Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Compare the number of words in each book \nfor i in range(len(books)):\n    print(\"There are {} words in {}.\".format(len(books[i].split()), book_files[i]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:05.161618Z","iopub.execute_input":"2025-02-18T22:35:05.161915Z","iopub.status.idle":"2025-02-18T22:35:05.384983Z","shell.execute_reply.started":"2025-02-18T22:35:05.161893Z","shell.execute_reply":"2025-02-18T22:35:05.384207Z"}},"outputs":[{"name":"stdout","text":"There are 361612 words in Anna_Karenina_by_Leo_Tolstoy.rtf.\nThere are 96185 words in The_Adventures_of_Tom_Sawyer_by_Mark_Twain.rtf.\nThere are 194282 words in The_Romance_of_Lust_by_Anonymous.rtf.\nThere are 53211 words in The_Prince_by_Nicolo_Machiavelli.rtf.\nThere are 30423 words in Alices_Adventures_in_Wonderland_by_Lewis_Carroll.rtf.\nThere are 163109 words in Emma_by_Jane_Austen.rtf.\nThere are 110213 words in The_Adventures_of_Sherlock_Holmes_by_Arthur_Conan_Doyle.rtf.\nThere are 480495 words in The_Count_of_Monte_Cristo_by_Alexandre_Dumas.rtf.\nThere are 113452 words in David_Copperfield_by_Charles_Dickens.rtf.\nThere are 25395 words in Metamorphosis_by_Franz_Kafka.rtf.\nThere are 126999 words in Pride_and_Prejudice_by_Jane_Austen.rtf.\nThere are 83657 words in The_Picture_of_Dorian_Gray_by_Oscar_Wilde.rtf.\nThere are 166996 words in Dracula_by_Bram_Stoker.rtf.\nThere are 165188 words in Oliver_Twist_by_Charles_Dickens.rtf.\nThere are 78912 words in Frankenstein_by_Mary_Shelley.rtf.\nThere are 105428 words in Grimms_Fairy_Tales_by_The_Brothers_Grimm.rtf.\nThere are 9463 words in The_Yellow_Wallpaper_by_Charlotte_Perkins_Gilman.rtf.\nThere are 433993 words in Don_Quixote_by_Miguel_de_Cervantes.rtf.\nThere are 191598 words in Great_Expectations_by_Charles_Dickens.rtf.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"def clean_text(text):\n    '''Remove unwanted characters and extra spaces from the text'''\n    text = re.sub(r'\\n', ' ', text) \n    text = re.sub(r'[{}@_*>()\\\\#%+=\\[\\]]','', text)\n    text = re.sub('a0','', text)\n    text = re.sub('\\'92t','\\'t', text)\n    text = re.sub('\\'92s','\\'s', text)\n    text = re.sub('\\'92m','\\'m', text)\n    text = re.sub('\\'92ll','\\'ll', text)\n    text = re.sub('\\'91','', text)\n    text = re.sub('\\'92','', text)\n    text = re.sub('\\'93','', text)\n    text = re.sub('\\'94','', text)\n    text = re.sub('\\.','. ', text)\n    text = re.sub('\\!','! ', text)\n    text = re.sub('\\?','? ', text)\n    text = re.sub(' +',' ', text)\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:07.832790Z","iopub.execute_input":"2025-02-18T22:35:07.833086Z","iopub.status.idle":"2025-02-18T22:35:07.838755Z","shell.execute_reply.started":"2025-02-18T22:35:07.833063Z","shell.execute_reply":"2025-02-18T22:35:07.837965Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Clean the text of the books\nclean_books = []\nfor book in books:\n    clean_books.append(clean_text(book))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:08.378139Z","iopub.execute_input":"2025-02-18T22:35:08.378384Z","iopub.status.idle":"2025-02-18T22:35:09.475276Z","shell.execute_reply.started":"2025-02-18T22:35:08.378363Z","shell.execute_reply":"2025-02-18T22:35:09.474596Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Check to ensure the text has been cleaned properly\nclean_books[0][:500]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:09.839957Z","iopub.execute_input":"2025-02-18T22:35:09.840233Z","iopub.status.idle":"2025-02-18T22:35:09.845305Z","shell.execute_reply.started":"2025-02-18T22:35:09.840211Z","shell.execute_reply":"2025-02-18T22:35:09.844505Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"'rtf1ansiansicpg1252cocoartf1404cocoasubrtf470 fonttblf0fmodernfcharset0 Courier; colortbl;red255green255blue255;red0green0blue0; margl1440margr1440vieww10800viewh8400viewkind0 deftab720 pardpardeftab720sl280partightenfactor0 f0fs24 cf2 expnd0expndtw0kerning0 outl0strokewidth0 strokec2 The Project Gutenberg EBook of Anna Karenina, by Leo Tolstoy This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the '"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"# Create a dictionary to convert the vocabulary (characters) to integers\nvocab_to_int = {}\ncount = 0\nfor book in clean_books:\n    for character in book:\n        \n        if character not in vocab_to_int:\n            vocab_to_int[character] = count\n            count += 1\n\n# Add special tokens to vocab_to_int\ncodes = ['<PAD>','<EOS>','<GO>']\nfor code in codes:\n    vocab_to_int[code] = count\n    count += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:10.112373Z","iopub.execute_input":"2025-02-18T22:35:10.112618Z","iopub.status.idle":"2025-02-18T22:35:11.296695Z","shell.execute_reply.started":"2025-02-18T22:35:10.112597Z","shell.execute_reply":"2025-02-18T22:35:11.296016Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# Check the size of vocabulary and all of the values\nvocab_size = len(vocab_to_int)\nprint(\"The vocabulary contains {} characters.\".format(vocab_size))\nprint(sorted(vocab_to_int))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:12.181249Z","iopub.execute_input":"2025-02-18T22:35:12.181540Z","iopub.status.idle":"2025-02-18T22:35:12.186795Z","shell.execute_reply.started":"2025-02-18T22:35:12.181510Z","shell.execute_reply":"2025-02-18T22:35:12.185917Z"}},"outputs":[{"name":"stdout","text":"The vocabulary contains 78 characters.\n[' ', '!', '\"', '$', '&', \"'\", ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<EOS>', '<GO>', '<PAD>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# Create another dictionary to convert integers to their respective characters\nint_to_vocab = {}\nfor character, value in vocab_to_int.items():\n    int_to_vocab[value] = character","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:12.458616Z","iopub.execute_input":"2025-02-18T22:35:12.458971Z","iopub.status.idle":"2025-02-18T22:35:12.462812Z","shell.execute_reply.started":"2025-02-18T22:35:12.458944Z","shell.execute_reply":"2025-02-18T22:35:12.461941Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Split the text from the books into sentences.\nsentences = []\nfor book in clean_books:\n    for sentence in book.split('. '):\n        sentences.append(sentence + '.')\nprint(\"There are {} sentences.\".format(len(sentences)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:14.688432Z","iopub.execute_input":"2025-02-18T22:35:14.688732Z","iopub.status.idle":"2025-02-18T22:35:14.774295Z","shell.execute_reply.started":"2025-02-18T22:35:14.688708Z","shell.execute_reply":"2025-02-18T22:35:14.773552Z"}},"outputs":[{"name":"stdout","text":"There are 131951 sentences.\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# Check to ensure the text has been split correctly.\nsentences[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:14.925524Z","iopub.execute_input":"2025-02-18T22:35:14.925820Z","iopub.status.idle":"2025-02-18T22:35:14.930756Z","shell.execute_reply.started":"2025-02-18T22:35:14.925796Z","shell.execute_reply":"2025-02-18T22:35:14.930025Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"['rtf1ansiansicpg1252cocoartf1404cocoasubrtf470 fonttblf0fmodernfcharset0 Courier; colortbl;red255green255blue255;red0green0blue0; margl1440margr1440vieww10800viewh8400viewkind0 deftab720 pardpardeftab720sl280partightenfactor0 f0fs24 cf2 expnd0expndtw0kerning0 outl0strokewidth0 strokec2 The Project Gutenberg EBook of Anna Karenina, by Leo Tolstoy This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever.',\n 'You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at http://www.',\n 'gutenberg.',\n 'org/license.',\n 'Title: Anna Karenina Author: Leo Tolstoy Release Date: July 01, 1998 EBook 1399 Reposted: April 02, 2005 corrections, reposted to new folder structure by David Widger Reposted: December 09, 2011 corrections Reposted: December 15, 2012 corrections, conversion to HTML Reposted: February 14, 2013 conversion to XHTML Strict by David Widger Reposted: February 22, 2013 corrections, conversion to RST by Andrew Sly Language: English Character set encoding: UTF-8 START OF THIS PROJECT GUTENBERG EBOOK ANNA KARENINA Produced by David Brannan.']"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"# Convert sentences to integers\nint_sentences = []\n\nfor sentence in sentences:\n    int_sentence = []\n    for character in sentence:\n        int_sentence.append(vocab_to_int[character])\n    int_sentences.append(int_sentence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:18.012808Z","iopub.execute_input":"2025-02-18T22:35:18.013094Z","iopub.status.idle":"2025-02-18T22:35:20.467610Z","shell.execute_reply.started":"2025-02-18T22:35:18.013072Z","shell.execute_reply":"2025-02-18T22:35:20.466959Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Find the length of each sentence\nlengths = []\nfor sentence in int_sentences:\n    lengths.append(len(sentence))\nlengths = pd.DataFrame(lengths, columns=[\"counts\"])\nlengths.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:20.468686Z","iopub.execute_input":"2025-02-18T22:35:20.468912Z","iopub.status.idle":"2025-02-18T22:35:20.533495Z","shell.execute_reply.started":"2025-02-18T22:35:20.468893Z","shell.execute_reply":"2025-02-18T22:35:20.532911Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"              counts\ncount  131951.000000\nmean      120.569787\nstd       116.983324\nmin         1.000000\n25%        46.000000\n50%        92.000000\n75%       160.000000\nmax      8906.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>counts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>131951.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>120.569787</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>116.983324</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>46.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>92.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>160.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>8906.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"# Limit the data we will use to train our model\nmax_length = 128\nmin_length = 10\n\ngood_sentences = []\n\nfor sentence in int_sentences:\n    if len(sentence) <= max_length and len(sentence) >= min_length:\n        good_sentences.append(sentence)\n\nprint(\"We will use {} to train and test our model.\".format(len(good_sentences)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:20.534856Z","iopub.execute_input":"2025-02-18T22:35:20.535101Z","iopub.status.idle":"2025-02-18T22:35:20.572066Z","shell.execute_reply.started":"2025-02-18T22:35:20.535082Z","shell.execute_reply":"2025-02-18T22:35:20.571164Z"}},"outputs":[{"name":"stdout","text":"We will use 79039 to train and test our model.\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# Split the data into training and testing sentences\ntraining, testing = train_test_split(good_sentences, test_size = 0.15, random_state = 2)\n\nprint(\"Number of training sentences:\", len(training))\nprint(\"Number of testing sentences:\", len(testing))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:33.473261Z","iopub.execute_input":"2025-02-18T22:35:33.473548Z","iopub.status.idle":"2025-02-18T22:35:33.500126Z","shell.execute_reply.started":"2025-02-18T22:35:33.473523Z","shell.execute_reply":"2025-02-18T22:35:33.499321Z"}},"outputs":[{"name":"stdout","text":"Number of training sentences: 67183\nNumber of testing sentences: 11856\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# Sort the sentences by length to reduce padding, which will allow the model to train faster\ntraining_sorted = []\ntesting_sorted = []\n\nfor i in range(min_length, max_length+1):\n    for sentence in training:\n        if len(sentence) == i:\n            training_sorted.append(sentence)\n    for sentence in testing:\n        if len(sentence) == i:\n            testing_sorted.append(sentence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:38.592551Z","iopub.execute_input":"2025-02-18T22:35:38.592865Z","iopub.status.idle":"2025-02-18T22:35:40.024408Z","shell.execute_reply.started":"2025-02-18T22:35:38.592840Z","shell.execute_reply":"2025-02-18T22:35:40.023722Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"def noise_maker(sentence, threshold):\n    '''Relocate, remove, or add characters to create spelling mistakes'''\n\n    letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n    \n    noisy_sentence = []\n    i = 0\n    while i < len(sentence):\n        random = np.random.uniform(0,1,1)\n        # Most characters will be correct since the threshold value is high\n        if random < threshold:\n            noisy_sentence.append(sentence[i])\n        else:\n            new_random = np.random.uniform(0,1,1)\n            # ~33% chance characters will swap locations\n            if new_random > 0.67:\n                if i == (len(sentence) - 1):\n                    # If last character in sentence, it will not be typed\n                    continue\n                else:\n                    # if any other character, swap order with following character\n                    noisy_sentence.append(sentence[i+1])\n                    noisy_sentence.append(sentence[i])\n                    i += 1\n            # ~33% chance an extra lower case letter will be added to the sentence\n            elif new_random < 0.33:\n                random_letter = np.random.choice(letters, 1)[0]\n                noisy_sentence.append(vocab_to_int[random_letter])\n                noisy_sentence.append(sentence[i])\n            # ~33% chance a character will not be typed\n            else:\n                pass     \n        i += 1\n    return noisy_sentence","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:40.025373Z","iopub.execute_input":"2025-02-18T22:35:40.025589Z","iopub.status.idle":"2025-02-18T22:35:40.035911Z","shell.execute_reply.started":"2025-02-18T22:35:40.025570Z","shell.execute_reply":"2025-02-18T22:35:40.035061Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# Check to ensure noise_maker is making mistakes correctly.\nthreshold = 0.9\nfor sentence in training_sorted[:5]:\n    print(sentence)\n    print(noise_maker(sentence, threshold))\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:42.955528Z","iopub.execute_input":"2025-02-18T22:35:42.955838Z","iopub.status.idle":"2025-02-18T22:35:42.963326Z","shell.execute_reply.started":"2025-02-18T22:35:42.955814Z","shell.execute_reply":"2025-02-18T22:35:42.962438Z"}},"outputs":[{"name":"stdout","text":"[58, 23, 8, 1, 7, 13, 5, 19, 52, 43]\n[23, 58, 8, 1, 7, 13, 5, 19, 52, 43]\n\n[39, 5, 7, 10, 24, 1, 20, 23, 41, 43]\n[13, 39, 5, 7, 10, 24, 1, 1, 20, 23, 41, 43]\n\n[33, 0, 23, 6, 7, 22, 23, 5, 1, 43]\n[33, 0, 8, 23, 6, 7, 22, 23, 5, 1, 43]\n\n[10, 16, 1, 23, 5, 17, 23, 0, 10, 43]\n[10, 16, 1, 24, 23, 5, 17, 16, 23, 0, 10, 43]\n\n[61, 19, 22, 23, 5, 41, 19, 7, 1, 43]\n[61, 19, 22, 23, 41, 19, 7, 1, 43]\n\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"def pad_sentence_batch(sentence_batch):\n    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n    max_sentence = max([len(sentence) for sentence in sentence_batch])\n    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:45.159687Z","iopub.execute_input":"2025-02-18T22:35:45.159977Z","iopub.status.idle":"2025-02-18T22:35:45.164220Z","shell.execute_reply.started":"2025-02-18T22:35:45.159953Z","shell.execute_reply":"2025-02-18T22:35:45.163271Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"def get_batches(sentences, batch_size, threshold):\n    \"\"\"Batch sentences, noisy sentences, and the lengths of their sentences together.\n       With each epoch, sentences will receive new mistakes\"\"\"\n    \n    for batch_i in range(0, len(sentences)//batch_size):\n        start_i = batch_i * batch_size\n        sentences_batch = sentences[start_i:start_i + batch_size]\n        \n        sentences_batch_noisy = []\n        for sentence in sentences_batch:\n            sentences_batch_noisy.append(noise_maker(sentence, threshold))\n            \n        sentences_batch_eos = []\n        for sentence in sentences_batch:\n            sentence.append(vocab_to_int['<EOS>'])\n            sentences_batch_eos.append(sentence)\n            \n        pad_sentences_batch = np.array(pad_sentence_batch(sentences_batch_eos))\n        pad_sentences_noisy_batch = np.array(pad_sentence_batch(sentences_batch_noisy))\n        \n        # Need the lengths for the _lengths parameters\n        pad_sentences_lengths = []\n        for sentence in pad_sentences_batch:\n            pad_sentences_lengths.append(len(sentence))\n        \n        pad_sentences_noisy_lengths = []\n        for sentence in pad_sentences_noisy_batch:\n            pad_sentences_noisy_lengths.append(len(sentence))\n        \n        yield pad_sentences_noisy_batch, pad_sentences_batch, pad_sentences_noisy_lengths, pad_sentences_lengths","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:45.516244Z","iopub.execute_input":"2025-02-18T22:35:45.516526Z","iopub.status.idle":"2025-02-18T22:35:45.521951Z","shell.execute_reply.started":"2025-02-18T22:35:45.516504Z","shell.execute_reply":"2025-02-18T22:35:45.521152Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass SpellCorrectionDataset(Dataset):\n    def __init__(self, sentences, vocab_to_int, noise_threshold=0.9):\n        \"\"\"\n        sentences: list of \"good\" sentences (each is a list of integer tokens)\n        vocab_to_int: dictionary mapping characters (and special tokens) to integers\n        noise_threshold: probability that a given character remains unchanged\n        \"\"\"\n        self.sentences = sentences\n        self.vocab_to_int = vocab_to_int\n        self.noise_threshold = noise_threshold\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, idx):\n        # Get the clean sentence (make a copy to avoid in-place modifications)\n        clean_sentence = self.sentences[idx][:]\n\n        # Append <EOS> token to the clean sentence if it isn't there already.\n        if clean_sentence[-1] != self.vocab_to_int['<EOS>']:\n            clean_sentence = clean_sentence + [self.vocab_to_int['<EOS>']]\n\n        # Generate a noisy version on the fly.\n        # Note: noise_maker should work with a sentence represented as a list of ints.\n        noisy_sentence = noise_maker(self.sentences[idx], self.noise_threshold)\n        \n        # You might want to also append an <EOS> token to the noisy sentence, \n        # depending on how your model handles sequence termination.\n        # For now, we leave it as is.\n\n        # Convert lists to torch tensors.\n        return (torch.tensor(noisy_sentence, dtype=torch.long),\n                torch.tensor(clean_sentence, dtype=torch.long))\n\n\ndef pad_collate_fn(batch):\n    \"\"\"\n    Collate function to pad sequences in the batch.\n    Each batch item is a tuple: (noisy_sentence, clean_sentence)\n    \"\"\"\n    # Unpack batch items.\n    noisy_batch, clean_batch = zip(*batch)\n\n    # Determine max lengths in the batch.\n    max_noisy_len = max([s.size(0) for s in noisy_batch])\n    max_clean_len = max([s.size(0) for s in clean_batch])\n    \n    # Pad noisy sequences.\n    padded_noisy = [\n        torch.cat([s, torch.tensor([vocab_to_int['<PAD>']] * (max_noisy_len - s.size(0)),\n                                     dtype=torch.long)])\n        for s in noisy_batch\n    ]\n    # Pad clean sequences.\n    padded_clean = [\n        torch.cat([s, torch.tensor([vocab_to_int['<PAD>']] * (max_clean_len - s.size(0)),\n                                     dtype=torch.long)])\n        for s in clean_batch\n    ]\n\n    # Optionally, collect the original lengths.\n    noisy_lengths = torch.tensor([s.size(0) for s in noisy_batch], dtype=torch.long)\n    clean_lengths = torch.tensor([s.size(0) for s in clean_batch], dtype=torch.long)\n    \n    # Stack into batch tensors.\n    padded_noisy = torch.stack(padded_noisy)\n    padded_clean = torch.stack(padded_clean)\n    \n    return padded_noisy, padded_clean, noisy_lengths, clean_lengths\n\n\n# Create your custom dataset instance\ndataset = SpellCorrectionDataset(training_sorted, vocab_to_int, noise_threshold=0.9)\n\n# Split the dataset into training and validation sets (e.g., 85% training, 15% validation)\ntrain_size = int(0.85 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n# Define batch size\nbatch_size = 128\n\n# Create DataLoaders for training and validation.\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate_fn)\n\n# Optional: Inspect one batch from the training loader\nfor batch in train_loader:\n    src, trg, src_lengths, trg_lengths = batch\n    print(\"Source batch shape:\", src.shape)\n    print(\"Target batch shape:\", trg.shape)\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:45.753413Z","iopub.execute_input":"2025-02-18T22:35:45.753745Z","iopub.status.idle":"2025-02-18T22:35:45.823584Z","shell.execute_reply.started":"2025-02-18T22:35:45.753717Z","shell.execute_reply":"2025-02-18T22:35:45.822928Z"}},"outputs":[{"name":"stdout","text":"Source batch shape: torch.Size([128, 130])\nTarget batch shape: torch.Size([128, 128])\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport random\n\n###############################################\n# Attention Module (Bahdanau-style Attention) #\n###############################################\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        # The attention layer will take the concatenation of the decoder's current hidden state\n        # and each encoder output, then output an energy scalar.\n        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n        self.v = nn.Linear(hidden_dim, 1, bias=False)\n        \n    def forward(self, decoder_hidden, encoder_outputs):\n        \"\"\"\n        Args:\n            decoder_hidden: [batch size, hidden_dim] -- current decoder hidden state (from the top layer)\n            encoder_outputs: [batch size, src_len, hidden_dim] -- all encoder hidden states\n            \n        Returns:\n            attention_weights: [batch size, src_len] -- normalized weights over the encoder outputs\n        \"\"\"\n        batch_size = encoder_outputs.shape[0]\n        src_len = encoder_outputs.shape[1]\n        \n        # Repeat decoder_hidden src_len times to concatenate with each encoder output.\n        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch, src_len, hidden_dim]\n        \n        # Concatenate and compute energy\n        energy = torch.tanh(self.attn(torch.cat((decoder_hidden, encoder_outputs), dim=2)))  # [batch, src_len, hidden_dim]\n        # Compute unnormalized attention scores\n        attention = self.v(energy).squeeze(2)  # [batch, src_len]\n        \n        # Normalize with softmax over src_len\n        return torch.softmax(attention, dim=1)\n\n#######################################\n# Encoder (Modified to return outputs)#\n#######################################\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        \n        self.embedding = nn.Embedding(input_dim, embedding_dim)\n        # batch_first=True: input shape [batch, seq_len, features]\n        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src):\n        \"\"\"\n        src: [batch size, src length]\n        Returns:\n            outputs: [batch size, src length, hidden_dim] (all hidden states)\n            hidden: [n_layers, batch size, hidden_dim]\n            cell: [n_layers, batch size, hidden_dim]\n        \"\"\"\n        embedded = self.dropout(self.embedding(src))  # [batch, src_len, embedding_dim]\n        outputs, (hidden, cell) = self.rnn(embedded)\n        return outputs, hidden, cell\n\n#####################################\n# Decoder (with Attention)          #\n#####################################\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n        super().__init__()\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        \n        self.embedding = nn.Embedding(output_dim, embedding_dim)\n        # The input to the LSTM now is the concatenation of the embedding and the context vector.\n        self.rnn = nn.LSTM(embedding_dim + hidden_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n        self.attention = Attention(hidden_dim)\n        # The fully-connected layer takes the concatenated [LSTM output; context] vector.\n        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input, hidden, cell, encoder_outputs):\n        \"\"\"\n        Args:\n            input: [batch size] - current token for each sentence in the batch\n            hidden: [n_layers, batch size, hidden_dim]\n            cell: [n_layers, batch size, hidden_dim]\n            encoder_outputs: [batch size, src length, hidden_dim]\n        \n        Returns:\n            prediction: [batch size, output_dim]\n            hidden: updated hidden state\n            cell: updated cell state\n        \"\"\"\n        # Add time dimension to input: [batch size] -> [batch size, 1]\n        input = input.unsqueeze(1)\n        embedded = self.dropout(self.embedding(input))  # [batch, 1, embedding_dim]\n        \n        # Compute attention weights using the top layer decoder hidden state (i.e., hidden[-1])\n        a = self.attention(hidden[-1], encoder_outputs)  # [batch, src_len]\n        a = a.unsqueeze(1)  # [batch, 1, src_len]\n        # Compute context vector as the weighted sum of encoder outputs.\n        context = torch.bmm(a, encoder_outputs)  # [batch, 1, hidden_dim]\n        \n        # Concatenate the embedded input and context vector.\n        rnn_input = torch.cat((embedded, context), dim=2)  # [batch, 1, embedding_dim + hidden_dim]\n        \n        # Pass through the LSTM\n        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n        # output: [batch, 1, hidden_dim]\n        output = output.squeeze(1)   # [batch, hidden_dim]\n        context = context.squeeze(1) # [batch, hidden_dim]\n        \n        # Concatenate output and context and pass through the final linear layer.\n        prediction = self.fc_out(torch.cat((output, context), dim=1))  # [batch, output_dim]\n        \n        return prediction, hidden, cell\n\n#####################################\n# Seq2Seq (Updated)                 #\n#####################################\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n        \n        # Ensure the encoder and decoder have matching dimensions and layers.\n        assert encoder.hidden_dim == decoder.hidden_dim, \"Hidden dimensions of encoder and decoder must be equal!\"\n        assert encoder.n_layers == decoder.n_layers, \"Encoder and decoder must have equal number of layers!\"\n\n    def forward(self, src, trg, teacher_forcing_ratio):\n        \"\"\"\n        src: [batch size, src length]\n        trg: [batch size, trg length]\n        teacher_forcing_ratio: probability to use teacher forcing\n        Returns:\n            outputs: [batch size, trg length, output_dim]\n        \"\"\"\n        batch_size = src.shape[0]\n        trg_length = trg.shape[1]\n        trg_vocab_size = self.decoder.output_dim\n        \n        # Tensor to store decoder outputs.\n        outputs = torch.zeros(batch_size, trg_length, trg_vocab_size).to(self.device)\n        \n        # Encoder forward pass returns all outputs.\n        encoder_outputs, hidden, cell = self.encoder(src)\n        \n        # The first token of each target sentence should be the <sos> token.\n        input = trg[:, 0]  # [batch size]\n        \n        for t in range(1, trg_length):\n            # Pass through the decoder one time step at a time, providing encoder_outputs for attention.\n            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n            outputs[:, t, :] = output\n            \n            # Decide whether to use teacher forcing.\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1)  # [batch size]\n            input = trg[:, t] if teacher_force else top1\n        \n        return outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:47.799804Z","iopub.execute_input":"2025-02-18T22:35:47.800097Z","iopub.status.idle":"2025-02-18T22:35:47.814740Z","shell.execute_reply.started":"2025-02-18T22:35:47.800074Z","shell.execute_reply":"2025-02-18T22:35:47.813769Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"PAD_IDX = vocab_to_int['<PAD>']\nSOS_IDX = vocab_to_int['<GO>']  # or '<SOS>' depending on your token naming\n\n# Hyperparameters (adjust as needed)\nINPUT_DIM = len(vocab_to_int)      # Size of the vocabulary\nOUTPUT_DIM = len(vocab_to_int)     # For seq2seq, input and output vocabs are often the same\nEMBEDDING_DIM = 96\nHIDDEN_DIM = 128\nN_LAYERS = 2\nDROPOUT = 0.65\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Instantiate the model components\nencoder = Encoder(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\ndecoder = Decoder(OUTPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\nmodel = Seq2Seq(encoder, decoder, device).to(device)\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:55.456001Z","iopub.execute_input":"2025-02-18T22:35:55.456279Z","iopub.status.idle":"2025-02-18T22:35:55.472985Z","shell.execute_reply.started":"2025-02-18T22:35:55.456257Z","shell.execute_reply":"2025-02-18T22:35:55.472286Z"}},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"import torch.optim as optim\nimport torch.nn as nn\nimport time\nfrom tqdm import tqdm  # Import tqdm for progress bars\n\n# Hyperparameters\nLEARNING_RATE = 0.001\nN_EPOCHS = 50\nCLIP = 1  # For gradient clipping\n\n# Define the optimizer and loss function\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\nPAD_IDX = vocab_to_int['<PAD>']\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n\ndef train(model, iterator, optimizer, criterion, clip, teacher_forcing_ratio):\n    \"\"\"\n    Performs one epoch of training.\n    \"\"\"\n    model.train()\n    epoch_loss = 0\n\n    # Wrap the iterator with tqdm for a progress bar\n    progress_bar = tqdm(iterator, desc=\"Training\", leave=False)\n    for batch in progress_bar:\n        src, trg, src_lengths, trg_lengths = batch\n        src, trg = src.to(device), trg.to(device)\n        \n        optimizer.zero_grad()\n        \n        # Forward pass: teacher forcing is applied based on the given ratio\n        output = model(src, trg, teacher_forcing_ratio)\n        # output shape: [batch size, trg length, output dim]\n        \n        # Reshape for loss computation\n        output_dim = output.shape[-1]\n        output = output.contiguous().view(-1, output_dim)   # [batch_size * trg_len, output_dim]\n        trg = trg.contiguous().view(-1)                      # [batch_size * trg_len]\n        \n        loss = criterion(output, trg)\n        loss.backward()\n        \n        # Clip gradients to prevent exploding gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        \n        optimizer.step()\n        epoch_loss += loss.item()\n        \n        # Optionally update the progress bar description with the current loss\n        progress_bar.set_postfix(loss=loss.item())\n    \n    return epoch_loss / len(iterator)\n\n\ndef evaluate(model, iterator, criterion):\n    \"\"\"\n    Evaluates the model on a validation set without teacher forcing.\n    \"\"\"\n    model.eval()\n    epoch_loss = 0\n\n    # Wrap the iterator with tqdm for a progress bar\n    progress_bar = tqdm(iterator, desc=\"Evaluating\", leave=False)\n    with torch.no_grad():\n        for batch in progress_bar:\n            src, trg, src_lengths, trg_lengths = batch\n            src, trg = src.to(device), trg.to(device)\n            \n            # Turn off teacher forcing during evaluation\n            output = model(src, trg, teacher_forcing_ratio=0)\n            \n            output_dim = output.shape[-1]\n            output = output.contiguous().view(-1, output_dim)\n            trg = trg.contiguous().view(-1)\n            \n            loss = criterion(output, trg)\n            epoch_loss += loss.item()\n            \n            progress_bar.set_postfix(loss=loss.item())\n    \n    return epoch_loss / len(iterator)\n\n\n# Example training loop with tqdm progress for each epoch\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n    start_time = time.time()\n    \n    # Use tqdm to track the epochs as well if desired.\n    train_loss = train(model, train_loader, optimizer, criterion, CLIP, teacher_forcing_ratio=0.2)\n    valid_loss = evaluate(model, val_loader, criterion)\n    \n    end_time = time.time()\n    epoch_mins, epoch_secs = divmod(int(end_time - start_time), 60)\n    \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'best-model.pt')\n        print(\"Model Saved!\")\n    \n    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T22:35:57.999051Z","iopub.execute_input":"2025-02-18T22:35:57.999362Z","iopub.status.idle":"2025-02-18T23:49:28.782196Z","shell.execute_reply.started":"2025-02-18T22:35:57.999333Z","shell.execute_reply":"2025-02-18T23:49:28.781037Z"}},"outputs":[{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Model Saved!\nEpoch: 01 | Time: 3m 29s\n\tTrain Loss: 3.052\n\t Val. Loss: 3.053\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Model Saved!\nEpoch: 02 | Time: 3m 29s\n\tTrain Loss: 2.875\n\t Val. Loss: 2.758\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Model Saved!\nEpoch: 03 | Time: 3m 29s\n\tTrain Loss: 2.508\n\t Val. Loss: 2.352\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Model Saved!\nEpoch: 04 | Time: 3m 29s\n\tTrain Loss: 2.292\n\t Val. Loss: 2.159\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch: 05 | Time: 3m 29s\n\tTrain Loss: 2.183\n\t Val. Loss: 2.257\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Model Saved!\nEpoch: 06 | Time: 3m 29s\n\tTrain Loss: 2.090\n\t Val. Loss: 2.026\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Model Saved!\nEpoch: 07 | Time: 3m 29s\n\tTrain Loss: 2.013\n\t Val. Loss: 1.998\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Model Saved!\nEpoch: 08 | Time: 3m 29s\n\tTrain Loss: 1.981\n\t Val. Loss: 1.977\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Model Saved!\nEpoch: 09 | Time: 3m 29s\n\tTrain Loss: 1.952\n\t Val. Loss: 1.952\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch: 10 | Time: 3m 29s\n\tTrain Loss: 1.894\n\t Val. Loss: 2.172\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch: 11 | Time: 3m 29s\n\tTrain Loss: 1.863\n\t Val. Loss: 2.016\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Model Saved!\nEpoch: 12 | Time: 3m 29s\n\tTrain Loss: 1.788\n\t Val. Loss: 1.951\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Model Saved!\nEpoch: 13 | Time: 3m 29s\n\tTrain Loss: 1.761\n\t Val. Loss: 1.881\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch: 14 | Time: 3m 29s\n\tTrain Loss: 1.709\n\t Val. Loss: 1.989\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch: 15 | Time: 3m 29s\n\tTrain Loss: 1.649\n\t Val. Loss: 2.006\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch: 16 | Time: 3m 29s\n\tTrain Loss: 1.600\n\t Val. Loss: 2.220\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch: 17 | Time: 3m 29s\n\tTrain Loss: 1.575\n\t Val. Loss: 1.948\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch: 18 | Time: 3m 29s\n\tTrain Loss: 1.543\n\t Val. Loss: 2.036\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch: 19 | Time: 3m 30s\n\tTrain Loss: 1.476\n\t Val. Loss: 2.189\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch: 20 | Time: 3m 29s\n\tTrain Loss: 1.468\n\t Val. Loss: 2.118\n","output_type":"stream"},{"name":"stderr","text":"                                                                      \r","output_type":"stream"},{"name":"stdout","text":"Epoch: 21 | Time: 3m 29s\n\tTrain Loss: 1.443\n\t Val. Loss: 2.130\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-52-827aa44e6b0a>\u001b[0m in \u001b[0;36m<cell line: 87>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# Use tqdm to track the epochs as well if desired.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-52-827aa44e6b0a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Clip gradients to prevent exploding gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":52},{"cell_type":"code","source":"model = Seq2Seq(encoder, decoder, device).to(device)\ndef predict_sentence(model, sentence, vocab_to_int, int_to_vocab, device, max_length=50):\n    \"\"\"\n    Generate a prediction for a single sentence using the attention-based seq2seq model.\n    \n    Args:\n        model (nn.Module): The trained Seq2Seq model with attention.\n        sentence (str): The noisy input sentence as a string.\n        vocab_to_int (dict): Mapping from tokens (e.g., characters) to their integer IDs.\n        int_to_vocab (dict): Mapping from integer IDs to tokens.\n        device (torch.device): Device on which to run the model.\n        max_length (int): Maximum length for the generated sentence.\n    \n    Returns:\n        str: The predicted (corrected) sentence as a string.\n    \"\"\"\n    model.eval()  # Set model to evaluation mode\n\n    # Convert the input sentence into token IDs.\n    # Only include tokens that exist in vocab_to_int.\n    token_ids = [vocab_to_int[char] for char in sentence if char in vocab_to_int]\n    \n    # Create a tensor and add a batch dimension: [1, src_len]\n    src_tensor = torch.LongTensor(token_ids).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        # Get the encoder outputs as well as the final hidden and cell states.\n        encoder_outputs, hidden, cell = model.encoder(src_tensor)\n    \n    # Initialize the decoder input with the <GO> token.\n    SOS_IDX = vocab_to_int['<GO>']\n    input_token = torch.LongTensor([SOS_IDX]).to(device)\n\n    predicted_tokens = []\n\n    for _ in range(max_length):\n        with torch.no_grad():\n            # Pass the current input token, hidden, cell, and encoder_outputs to the decoder.\n            output, hidden, cell = model.decoder(input_token, hidden, cell, encoder_outputs)\n        \n        # Get the token with the highest probability.\n        top1 = output.argmax(1).item()\n\n        # If the <EOS> token is predicted, stop decoding.\n        if top1 == vocab_to_int['<EOS>']:\n            break\n\n        predicted_tokens.append(top1)\n        # Update the input token for the next time step.\n        input_token = torch.LongTensor([top1]).to(device)\n    \n    # Convert the list of token IDs back to a string.\n    predicted_sentence = ''.join([int_to_vocab[token] for token in predicted_tokens])\n    return predicted_sentence\n\n# Example usage:\nnoisy_sentence = \"Ths is an exmple sentnce.\"\nprediction = predict_sentence(model, noisy_sentence, vocab_to_int, int_to_vocab, device)\nprint(\"Predicted sentence:\", prediction)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T23:49:35.143155Z","iopub.execute_input":"2025-02-18T23:49:35.143430Z","iopub.status.idle":"2025-02-18T23:49:35.173819Z","shell.execute_reply.started":"2025-02-18T23:49:35.143407Z","shell.execute_reply":"2025-02-18T23:49:35.173153Z"}},"outputs":[{"name":"stdout","text":"Predicted sentence: es is expleentcccceccec\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}